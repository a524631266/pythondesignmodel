{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'D:\\green\\spark-2.2.1-bin-2.6.0-cdh5.14.2' \n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"spark_read_from_hive\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-------+----+\n",
      "|      City|State|Country|IATA|\n",
      "+----------+-----+-------+----+\n",
      "|Abbotsford|   BC| Canada| YXX|\n",
      "|  Aberdeen|   SD|    USA| ABR|\n",
      "|   Abilene|   TX|    USA| ABI|\n",
      "|     Akron|   OH|    USA| CAK|\n",
      "|   Alamosa|   CO|    USA| ALS|\n",
      "+----------+-----+-------+----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------+--------------+----------+------+----+---------+---------+--------+-------------+-------------+-------------------+---------+--------------+-------------------+\n",
      "|    flight_date|unique_carrier|flight_num|origin|dest|arr_delay|cancelled|distance|carrier_delay|weather_delay|late_aircraft_delay|nas_delay|security_delay|actual_elapsed_time|\n",
      "+---------------+--------------+----------+------+----+---------+---------+--------+-------------+-------------+-------------------+---------+--------------+-------------------+\n",
      "|02/01/2015 0:00|            AA|         1|   JFK| LAX|      -19|        0|    2475|         null|         null|               null|     null|          null|                381|\n",
      "|03/01/2015 0:00|            AA|         1|   JFK| LAX|      -39|        0|    2475|         null|         null|               null|     null|          null|                358|\n",
      "|04/01/2015 0:00|            AA|         1|   JFK| LAX|      -12|        0|    2475|         null|         null|               null|     null|          null|                385|\n",
      "|05/01/2015 0:00|            AA|         1|   JFK| LAX|       -8|        0|    2475|         null|         null|               null|     null|          null|                389|\n",
      "|06/01/2015 0:00|            AA|         1|   JFK| LAX|       25|        0|    2475|            0|            0|                  0|       25|             0|                424|\n",
      "+---------------+--------------+----------+------+----+---------+---------+--------+-------------+-------------+-------------------+---------+--------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取机场数据\n",
    "'''\n",
    "如果hdfs上的数据是按照一定的分隔符结构化数据，是可以直接使用功能read.csv读取的\n",
    "'''\n",
    "airports = spark.read.csv(\n",
    "    'hdfs://cdh03:/input/airport-codes-na.csv',\n",
    "    header='true',\n",
    "    inferSchema='true',\n",
    "    sep='\\t'\n",
    ")\n",
    "\n",
    "airports.show(5)\n",
    "\n",
    "# 读取起飞延迟数据\n",
    "flightPref = spark.read.csv(\n",
    "    'hdfs://cdh03:/input/departuredelays.csv',\n",
    "    header='true'\n",
    ")\n",
    "\n",
    "flightPref.show(5)\n",
    "\n",
    "# 读取完整的宽表数据\n",
    "usa_flight = spark.read.csv(\n",
    "    'hdfs://cdh03:/input/usa_flights.csv',\n",
    "    header='true'\n",
    ")\n",
    "usa_flight.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+--------+\n",
      "|   City|origin|  Delays|\n",
      "+-------+------+--------+\n",
      "|Seattle|   SEA|159086.0|\n",
      "|Spokane|   GEG| 12404.0|\n",
      "|  Pasco|   PSC|   949.0|\n",
      "+-------+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 使用sql方式进行开发\n",
    "# 注册为临时视图\n",
    "airports.createOrReplaceTempView(\"airport\")\n",
    "\n",
    "# 注册为临时视图\n",
    "flightPref.createOrReplaceTempView('flight')\n",
    "\n",
    "\n",
    "# 联合两个视图数据，查询华盛顿州，各个城市的各家航空公司，起飞航班延迟的总数\n",
    "spark.sql('''\n",
    "select a.City,f.origin,sum(f.delay) as Delays \n",
    "   from flight f\n",
    "   join airport a\n",
    "   on a.IATA = f.origin\n",
    "   where a.State = 'WA'\n",
    "   group by a.City,f.origin\n",
    "   order by sum(f.delay) desc\n",
    "''').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flight_date: string (nullable = true)\n",
      " |-- unique_carrier: string (nullable = true)\n",
      " |-- flight_num: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- carrier_delay: string (nullable = true)\n",
      " |-- weather_delay: string (nullable = true)\n",
      " |-- late_aircraft_delay: string (nullable = true)\n",
      " |-- nas_delay: string (nullable = true)\n",
      " |-- security_delay: string (nullable = true)\n",
      " |-- actual_elapsed_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 一般练习\n",
    "import time\n",
    "\n",
    "# 查看元数据信息\n",
    "usa_flight.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(flight_date,StringType,true),StructField(unique_carrier,StringType,true),StructField(flight_num,StringType,true),StructField(origin,StringType,true),StructField(dest,StringType,true),StructField(arr_delay,StringType,true),StructField(cancelled,StringType,true),StructField(distance,StringType,true),StructField(carrier_delay,StringType,true),StructField(weather_delay,StringType,true),StructField(late_aircraft_delay,StringType,true),StructField(nas_delay,StringType,true),StructField(security_delay,StringType,true),StructField(actual_elapsed_time,StringType,true)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_flight.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flight_date',\n",
       " 'unique_carrier',\n",
       " 'flight_num',\n",
       " 'origin',\n",
       " 'dest',\n",
       " 'arr_delay',\n",
       " 'cancelled',\n",
       " 'distance',\n",
       " 'carrier_delay',\n",
       " 'weather_delay',\n",
       " 'late_aircraft_delay',\n",
       " 'nas_delay',\n",
       " 'security_delay',\n",
       " 'actual_elapsed_time']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_flight.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201664"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usa_flight.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\session.py:336: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "|  5|  Bob|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 手动创建一个dataframe\n",
    "d = [{'name':'Alice','age':1},{'name':'Bob','age':5}]\n",
    "\n",
    "df = spark.createDataFrame(d)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|age_value| name|\n",
      "+---------+-----+\n",
      "|        1|Alice|\n",
      "|        5|  Bob|\n",
      "+---------+-----+\n",
      "\n",
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('age','name')\n",
    "df.select(df['age'].alias('age_value'),'name').show()\n",
    "df.filter(df['name']=='Alice').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "| name|max(age)|\n",
      "+-----+--------+\n",
      "|  Bob|       5|\n",
      "|Alice|       1|\n",
      "+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions \n",
    "# 增加列\n",
    "df.select(df.age +1,'age','name')\n",
    "\n",
    "df.select(functions.lit(0).alias('id'),'age','name')\n",
    "\n",
    "# 采用row numbuer\n",
    "win = Window.partitionBy().orderBy(df['age'])\n",
    "df.withColumn(\"id\",functions.row_number().over(win)).select('id','name','age')\n",
    "\n",
    "df.groupBy('name').agg(functions.max(df['age'])).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flight_date: string (nullable = true)\n",
      " |-- unique_carrier: string (nullable = true)\n",
      " |-- flight_num: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- arr_delay: string (nullable = true)\n",
      " |-- cancelled: string (nullable = true)\n",
      " |-- distance: string (nullable = true)\n",
      " |-- carrier_delay: string (nullable = true)\n",
      " |-- weather_delay: string (nullable = true)\n",
      " |-- late_aircraft_delay: string (nullable = true)\n",
      " |-- nas_delay: string (nullable = true)\n",
      " |-- security_delay: string (nullable = true)\n",
      " |-- actual_elapsed_time: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usa_flight.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------------------+\n",
      "|unique_carrier|flight_num|          avg_time|\n",
      "+--------------+----------+------------------+\n",
      "|            AA|       269|213.92307692307693|\n",
      "|            AA|      1046| 93.84615384615384|\n",
      "|            AA|      1255|209.23076923076923|\n",
      "|            AA|      1670| 159.6153846153846|\n",
      "|            AA|      2372|190.53846153846155|\n",
      "|            AS|       548| 98.65384615384616|\n",
      "|            AS|       697| 67.54545454545455|\n",
      "|            AS|       880|345.46153846153845|\n",
      "|            AS|       496|             157.8|\n",
      "|            B6|       288|             312.0|\n",
      "|            B6|       603| 356.4166666666667|\n",
      "|            B6|       616| 321.2857142857143|\n",
      "|            B6|      1025|191.72727272727272|\n",
      "|            B6|      1274| 98.92307692307692|\n",
      "|            DL|       492|193.08333333333334|\n",
      "|            DL|       753|136.66666666666666|\n",
      "|            DL|      1128|  75.6923076923077|\n",
      "|            DL|      1618|  96.3076923076923|\n",
      "|            DL|      1717|              94.0|\n",
      "|            DL|        40|             145.1|\n",
      "+--------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#usa_flight.select(functions.split('flight_date','/')[2]).show(5)\n",
    "\n",
    "# 重新处理表结构\n",
    "usa_flight.select(functions.split(functions.split('flight_date','/')[2],' ')[0].alias('year'),\n",
    "                  functions.split('flight_date','/')[1].alias('month'),\n",
    "                  functions.split('flight_date','/')[1].alias('day'),\n",
    "                  functions.split(functions.split('flight_date','/')[2],' ')[1].alias('min'),\n",
    "                 ) #.show(5)\n",
    "\n",
    "\n",
    "# 计算有多少家航空公司\n",
    "usa_flight.agg(functions.countDistinct('unique_carrier')) #.show(5)\n",
    "\n",
    "# 计算每家航空公司各有多少个航班\n",
    "usa_flight.groupBy('unique_carrier').agg(functions.countDistinct('flight_num').alias('num')) #.show()\n",
    "\n",
    "# 计算每家航空公司各有多少个航班\n",
    "a = usa_flight.groupBy('unique_carrier','flight_num').agg(functions.count('flight_num').alias('num')) #.show()\n",
    "# 辅助验证\n",
    "a.filter(a['unique_carrier']  == 'UA').orderBy(a['num'].desc()) # .show()\n",
    "\n",
    "usa_flight.groupBy('unique_carrier','flight_num') \\\n",
    ".agg(functions.count('flight_num').alias('num')) \\\n",
    ".filter(usa_flight['unique_carrier']=='UA') \\\n",
    "#.show()\n",
    "\n",
    "# 利用窗口函数来处理\n",
    "win1 = Window.partitionBy('unique_carrier').orderBy(a['num'].desc())\n",
    "\n",
    "# 计算各家航班延迟最高的前5个班次\n",
    "a.select('unique_carrier','flight_num','num',functions.row_number().over(win1).alias('rank')) \\\n",
    ".where(functions.col('rank') <= 10) \\\n",
    "#.show()\n",
    "\n",
    "\n",
    "#计算始发城市延迟的最多的班次的城市\n",
    "usa_flight.groupBy('origin').agg(functions.count(functions.lit(1)).alias('num')).orderBy(functions.col('num').desc()) #.show()\n",
    "\n",
    "\n",
    "# 计算每个航空公司的每个航班的平均实际花费的时间\n",
    "usa_flight.groupBy('unique_carrier','flight_num') \\\n",
    " .agg(functions.avg('actual_elapsed_time').alias('avg_time')) \\\n",
    " .show()\n",
    " #.where(functions.col('unique_carrier')=='AA') \\\n",
    " #.filter(usa_flight['flight_num']=='1') \\\n",
    " #.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pyspark的udf官方文档](https://docs.databricks.com/spark/latest/spark-sql/udf-python.html#use-udf-with-dataframes)\n",
    "\n",
    "\n",
    "*语法格式*\n",
    "```python\n",
    "def funx(s):\n",
    "   return xxx\n",
    "\n",
    "spark.udf.register('自定义函数名',funx,返回类型【可选】)\n",
    "``` \n",
    "\n",
    "## 案例\n",
    "```python\n",
    "def squared(s):\n",
    "  return s * s\n",
    "spark.udf.register(\"squaredWithPython\", squared)\n",
    "```\n",
    "可选的可以定义函数的返回类型，默认是StringType\n",
    "```python\n",
    "from pyspark.sql.types import LongType\n",
    "def squared_typed(s):\n",
    "  return s * s\n",
    "spark.udf.register(\"squaredWithPython\", squared_typed, LongType())\n",
    "```\n",
    "## 在sql中调用\n",
    "```sql\n",
    "spark.range(1, 20).registerTempTable(\"test\")\n",
    "sql select id, squaredWithPython(id) as id_squared from test\n",
    "```\n",
    "\n",
    "## 在udf中调用\n",
    "```python\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import LongType\n",
    "squared_udf = udf(squared, LongType())\n",
    "df = spark.table(\"test\")\n",
    "display(df.select(\"id\", squared_udf(\"id\").alias(\"id_squared\")))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType\n",
    "# 使用自定义函数\n",
    "def selfRpund(x):\n",
    "    return  round(x,2)\n",
    "\n",
    "# 注册自定义函数\n",
    "spark.udf.register(\"self_round\",selfRpund,LongType())\n",
    "\n",
    "usa_flight.createOrReplaceTempView('ua')\n",
    "\n",
    "# 使用sql方式进行调用\n",
    "spark.sql(\"\"\"\n",
    "select \n",
    "\"\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
